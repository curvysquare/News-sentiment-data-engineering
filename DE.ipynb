{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaiaLens Data Engineering Task\n",
    "Author: Rhys Cooper\n",
    "\n",
    "### Summary:\n",
    "\n",
    "This code extracts news sentiment data for Tesla Inc between 25-02-2023 and 25-08-2023 using GaiaLensâ€™ news API, saves it as 'Tesla_newssent_extaction.csv' and then aggregates it. Before aggregating, a 5% decay for each month that has passed from the current date is applied to the sentiment score. Then the downweighted scores are averaged per required metric column specified in 'sentiment_columns.csv' and saved as a CSV called 'telsa_newssent_aggregation.csv'. The final output is therefore a dataframe with companyname and all the sentiment columns where each of the sentiment column is a mean over down-weighted values across all the dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1:  Prerequisites\n",
    "- Import required packages.\n",
    "- Load given CSV's into preliminary dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#load provided csvs as dataframes \n",
    "dates_tesla_df = pd.read_csv(filepath_or_buffer= '/Users/rhyscooper/Downloads/dates_tesla.csv')\n",
    "sentiment_columns_df = pd.read_csv(filepath_or_buffer='/Users/rhyscooper/Downloads/sentiment_columns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data extraction tools defintions\n",
    "- Defines the 'data extraction statistics' class with a primary function of calculating all the dates in the data range.\n",
    "- Defines the 'api call' used to call the Gaia Lens API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class extracts the relevant statistics required for the data extraction.\n",
    "# the \"get_date_range\" method creates a list of all dates between the first and last date of the 'dates_tesla_csv'.\n",
    "class data_extraction_stats():\n",
    "    def __init__(self, dates_df) -> None:\n",
    "        self.first_date = dates_df.iloc[0, 0]\n",
    "        self.last_date = dates_df.iloc[-1, 0]\n",
    "        self.get_date_range()\n",
    "        \n",
    "    def get_date_range(self):\n",
    "        first_date_DTD = datetime.datetime.strptime(self.first_date, '%d-%m-%Y').date()\n",
    "        last_date_DTD = datetime.datetime.strptime(self.last_date, '%d-%m-%Y').date()\n",
    "        # +1 added below so end date is included in the list\n",
    "        self.date_list = [first_date_DTD + datetime.timedelta(days=x) for x in range((last_date_DTD  - first_date_DTD).days + 1)]\n",
    "        self.date_list = [date.strftime('%Y-%m-%d') for date in self.date_list]\n",
    "        self.possible_dates = len(self.date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This class houses the api call meta data and call functionality. The 'push call' method takes 'date' as input argument and returns the response from the GaiaLens news API. \n",
    "# Any failed push calls are excepted and the failed date returned. User required to input their 'X-RapidAPI-Key'.\n",
    "class API_call():\n",
    "    def __init__(self) -> None:\n",
    "        self.url  = \"https://gaialens-esg-news.p.rapidapi.com/news\"\n",
    "        self.headers = {\"X-RapidAPI-Key\": '',\n",
    "\t                    \"X-RapidAPI-Host\": \"gaialens-esg-news.p.rapidapi.com\"}\n",
    "    \n",
    "    def push_call(self, date):\n",
    "        self.querystring = {\"isin\":\"US88160R1014\",\"date\":date}\n",
    "        try:\n",
    "            response = requests.get(self.url, headers=self.headers, params=self.querystring)\n",
    "            return response\n",
    "        except Exception:\n",
    "            print(\"date\", date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Main data extractor class defintion\n",
    "- This 'data_extractor' is a multi-inhertiance child class from the aforementioned 'data_extraction_stats' and 'API_call' parents classes.\n",
    "- It collects all articles for all available dates, turn them into dataframe rows and concatenate them into an overall dataframe which is saved as a CSV.\n",
    "- Includes a validation check to ensure the dataframe is of expected dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class data_extractor(data_extraction_stats, API_call):\n",
    "    def __init__(self, dates_DF, Filepath) -> None:\n",
    "        data_extraction_stats.__init__(self, dates_df = dates_DF)\n",
    "        API_call.__init__(self)\n",
    "        self.news_data = {}\n",
    "        self.filepath = Filepath\n",
    "\n",
    "    # This method stores the responses for all possible dates in the 'news_data' attribute.\n",
    "    # Converting the response into json format will not work if there is no articles available for that date. This is excepted\n",
    "    # and the empty dates are counted. \n",
    "    \n",
    "    def call_for_all_dates(self):\n",
    "        self.empty_dates = []\n",
    "        for date in self.date_list:\n",
    "            response_raw = self.push_call(date)\n",
    "            try:\n",
    "                response =response_raw.json()\n",
    "            except Exception:\n",
    "                self.empty_dates.append(date)   \n",
    "            if isinstance(response, dict):\n",
    "                self.news_data[date] = [response]\n",
    "            else: \n",
    "                self.news_data[date] = response\n",
    "                  \n",
    "        self.n_empty_dates = len(self.empty_dates)\n",
    "        self.filled_dates = len(self.date_list) - len(self.empty_dates)\n",
    "    \n",
    "   # each article for each date is constructed into a row of the dataframe. ie multiple rows for each article per date.     \n",
    "    def get_per_date_df_rows(self):\n",
    "        self.per_date_rows = {date: [] for date in self.news_data.keys()}\n",
    "        for date in self.news_data.keys():\n",
    "            day_articles = self.news_data[date]\n",
    "            for article_Num in range(len(day_articles)):\n",
    "                article = day_articles[article_Num]\n",
    "                article_df = pd.DataFrame([article])\n",
    "                self.per_date_rows[date].append(article_df)\n",
    "    \n",
    "    #unpacks the date:articles dictionary and stores all rows in a list, which are then concatenated to form the overall dataframe.               \n",
    "    def construct_df(self):\n",
    "        all_rows = []\n",
    "        for dfs in self.per_date_rows.values():\n",
    "            for df in dfs:\n",
    "                all_rows.append(df)\n",
    "        self.overall_df = pd.concat(all_rows, axis='rows')\n",
    "\n",
    "    # validates the true availabe dates (all possible minus responces recieved for) against the number of unique dates in the dataframe\n",
    "    # Also checks the number of rows in the dataframe is equal to the total amount of articles available across the data range.\n",
    "    def validation_check(self):\n",
    "\n",
    "        true_n_days = len(self.date_list) - self.n_empty_dates\n",
    "        provided_n_days = self.overall_df['date'].nunique()\n",
    "        print(true_n_days == provided_n_days)\n",
    "        \n",
    "        true_rows = sum([len(articles) for articles in self.news_data.values()])\n",
    "        \n",
    "        self.correct_shape = true_n_days ==provided_n_days and true_rows == self.overall_df.shape[0]\n",
    "        \n",
    "        return true_rows\n",
    "    \n",
    "    # the below method returns statements describing the success of the data extraction and if the resultant dataframe is of the expected dimensions.\n",
    "    def report_extractions_success(self):\n",
    "        expected_rows = self.validation_check()\n",
    "        print(\"Extracted data for {suc} out of {tot} days as empty data for a total of {fail} days.\".format(suc = self.filled_dates, tot = self.possible_dates, fail = self.n_empty_dates))\n",
    "        if self.correct_shape:\n",
    "            print(\"\\n\", \"The returned dataframe is of the expected dimensions of\", self.overall_df.shape)\n",
    "        \n",
    "        else:\n",
    "            print(\"\\n\", \"The returned dataframe is not of the expected dimensions of 60 by\", expected_rows, \"but instead\", self.overall_df.shape)\n",
    "    \n",
    "    def run_all(self):\n",
    "        self.call_for_all_dates()\n",
    "        self.get_per_date_df_rows()\n",
    "        self.construct_df()\n",
    "        self.report_extractions_success()\n",
    "        self.save_as_csv()       \n",
    "            \n",
    "    def save_as_csv(self):\n",
    "        self.overall_df.to_csv(self.filepath + 'Tesla_newssent_extraction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Run Date extraction\n",
    "- User should replace Filepath variable with their own. This w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Extracted data for 167 out of 182 days as empty data for a total of 15 days.\n",
      "\n",
      " The returned dataframe is of the expected dimensions of (1312, 60)\n"
     ]
    }
   ],
   "source": [
    "#create class instance and run all methods.\n",
    "de = data_extractor(dates_DF= dates_tesla_df, Filepath='/Users/rhyscooper/')\n",
    "de.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Data Aggregation class definition\n",
    "- Main purpose of this class is to aggregate the data included in the data extraction object.\n",
    "- It down weights the sentiment scores by 5% for every month that has passed from the current date.\n",
    "- Only the mean values for the columns present in the sentiment_columns are cacluated, as per the brief.\n",
    "- Non required columns are given a mean value of Nan and are subsquently dropped.\n",
    "- Includes a validation test to check the sentiment values have been down weighted correctly, and the resultant dataframe only includes the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data aggregation class does not inherit from the data extraction class but does have the data extraction class instance saved as a class attribute.\n",
    "class data_agg():\n",
    "    def __init__(self, sent_cols_df, Data_extractor_obj) -> None:\n",
    "        self.sent_cols = sent_cols_df\n",
    "        self.DE_obj = Data_extractor_obj\n",
    "        self.date_list =Data_extractor_obj.date_list\n",
    "        self.current_date = datetime.date.today()\n",
    "        self.discount_perc = 0.05\n",
    "        self.pre_calc_weights()\n",
    "    \n",
    "    # Calculates the amount of full, 30 day length months that has elapsed between the inputted date and the current date. \n",
    "    def elapsed_months(self, date):\n",
    "        date_dt = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        date_diff = self.current_date - date_dt\n",
    "        elapsed_full_months = date_diff.days // 30\n",
    "        \n",
    "        return elapsed_full_months    \n",
    "\n",
    "    # To reduce run time, each down weight is pre-calculated for every date in the data range and stored in the weight dictionary.\n",
    "    def pre_calc_weights(self):\n",
    "        self.weight_dict = {}\n",
    "        self.non_metric_cols = list(self.DE_obj.overall_df.columns[:5])\n",
    "        self.metric_cols = list((self.sent_cols.iloc[:,0]))\n",
    "        self.all_cols = list(self.DE_obj.overall_df.columns)\n",
    "        for date in self.date_list:\n",
    "            elapsed_full_months = self.elapsed_months(date)\n",
    "            down_weight = 1- elapsed_full_months*self.discount_perc\n",
    "            self.weight_dict[date] = down_weight\n",
    "\n",
    "    # Function to be used in the 'apply_discounter_to_df' method below that discounts the value in each row by the the down weight.\n",
    "    # 'metric_cols' only include numeric/string values, whereas 'non_metric_cols' includes companyname, date etc.\n",
    "    def discounter(self, row):\n",
    "        weight = self.weight_dict[row['date']]\n",
    "        for col in row.index:\n",
    "            if col in self.metric_cols and col not in self.non_metric_cols:\n",
    "                    row[col] = row[col] * weight if not isinstance(row[col], (str)) else row[col]\n",
    "        return row            \n",
    "                \n",
    "    def apply_discounter_to_df(self):\n",
    "        df = self.DE_obj.overall_df.copy()\n",
    "        df = df.apply(self.discounter, axis=1)\n",
    "        self.DA_overall = df\n",
    "    \n",
    "    # Below method checks that the downweighted values present in the data aggregation dataframe equal the expected weighted down value based on its corresponding\n",
    "    # value in the data extraction dataframe. \n",
    "    # It also checks the included columns in the final dataframe equal the specified columns to be included.  \n",
    "    def DA_validation_check(self):\n",
    "        \n",
    "        df_DE = self.DE_obj.overall_df.copy()\n",
    "        df_DE.reset_index(drop=True, inplace=True)\n",
    "        df_DA = self.DA_overall.copy()\n",
    "        df_DA.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Selects the first specified sentiment columns to test (GHG_emissions). The first non-zero metric score in the column is used in the test. \n",
    "        first_sent_col = self.sent_cols.iloc[0,0]\n",
    "        DE_first_index = df_DE.loc[df_DE[first_sent_col] != 0].first_valid_index()\n",
    "        DE_first_value =  df_DE[first_sent_col].loc[DE_first_index]\n",
    "        DE_first_date = df_DE['date'].iloc[DE_first_index]\n",
    "        \n",
    "        DA_first_value = df_DA[first_sent_col].iloc[DE_first_index]\n",
    "\n",
    "        test_elapsed_months= self.elapsed_months(DE_first_date)\n",
    "        down_weight = self.weight_dict[DE_first_date]\n",
    "        expected_value = DE_first_value * down_weight\n",
    "        \n",
    "        # Since final data aggregation dataframe only includes companyname and the sentiment columns included in the provided 'sentiment columns.csv', they form the the expected columns.\n",
    "        expected_cols = ['companyname'] + self.metric_cols\n",
    "        returned_cols = list(self.DA_mn_only_sent.columns)\n",
    "        \n",
    "        # If both tests are passed, prints comments describing the tests and demonstrates the down weighting equation being applied correctly.\n",
    "        if all([expected_value == DA_first_value, expected_cols ==  returned_cols]):\n",
    "            print(\"Returned columns match the\", len(expected_cols), 'specified columns.')\n",
    "            print('Down weighting has been applied correctly where a score of {DEFV} from {TEM} complete months ago is weighted down by {W} to {DAFV}'.format(DEFV = DE_first_value,TEM = test_elapsed_months,  W=down_weight, DAFV = DA_first_value))\n",
    "            \n",
    "        return all([expected_value == DA_first_value, expected_cols ==  returned_cols])\n",
    "\n",
    "    # Firstly calculates the mean sentiment scores for all columns. If sentiment column isnt in the required columns, a value of nan is used as the mean.\n",
    "    # Mean values per column, including nan, are stored in a dicitonary that is then turned into the dataframe of the mean values per all columns.\n",
    "    def calc_mean_sents(self):\n",
    "        all_cols =  {col: np.nan  for col in list(self.DA_overall.columns)}\n",
    "\n",
    "        for col in self.DA_overall.columns:\n",
    "            if col in self.metric_cols and col not in self.non_metric_cols:\n",
    "                all_cols[col] = round(self.DA_overall[col].mean(), 5)\n",
    "                \n",
    "        #company name added as the first column according to the specification. \n",
    "        all_cols['companyname'] = self.DA_overall['companyname'].iloc[0]\n",
    "        self.DA_mn = pd.DataFrame([all_cols])\n",
    "\n",
    "    # This method defines all the required columns (company name + specified sentiment columns) and returns the dataframe only with these columns, effectivley dropping the non-required columns.\n",
    "    def drop_columns(self):\n",
    "        self.required_info = ['companyname']\n",
    "        self.required_info.extend(self.metric_cols)\n",
    "        \n",
    "        self.DA_mn_only_sent = self.DA_mn[[col for col in self.required_info]]\n",
    "    # User sh       \n",
    "    def DA_save(self):\n",
    "        self.DA_mn_only_sent.to_csv(self.DE_obj.filepath + 'Tesla_newssent_aggregation.csv', index=False)\n",
    " \n",
    "    def run_all(self):\n",
    "        self.apply_discounter_to_df()\n",
    "        self.calc_mean_sents()\n",
    "        self.drop_columns()\n",
    "        if self.DA_validation_check():\n",
    "            self.DA_save()\n",
    "        else:\n",
    "            print(\"error in data aggregation\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Run data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned columns match the 49 specified columns.\n",
      "Down weighting has been applied correctly where a score of 0.5 from 9 complete months ago is weighted down by 0.55 to 0.275\n"
     ]
    }
   ],
   "source": [
    "da = data_agg(sentiment_columns_df, de)\n",
    "da.run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
